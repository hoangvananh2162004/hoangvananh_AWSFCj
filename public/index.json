[
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Extending deployment pipelines with Amazon ECS blue green deployments and lifecycle hooks. by Olly Pomeroy | on 11 SEP 2025 | in Amazon Elastic Container Service, Containers | Permalink\nThe blue/green deployment enables you to release applications by shifting traffic between two identical environments running different versions of the application. This allows you to mitigate common risks associated with deploying software, because you can roll back to a previous deployment instantly. Amazon Elastic Container Service (Amazon ECS) recently announced native support for blue/green deployments, removing the need to manage and integrate with other deployment tools. As part of this release, Amazon ECS has also introduced lifecycle hooks for deployments. Lifecycle hooks allow you to integrate test suites, manual approvals, and metrics into your deployment pipeline. In this post we dive into lifecycle hooks and show how they can be integrated into deployment workflows.\nBackground When an Amazon ECS service is created or updated, we create an immutable object of its exact specification, known as an Amazon ECS service revision. Then, the control plane attempts to deploy this service revision through an Amazon ECS service deployment. A deployment goes through multiple lifecycle states, such as IN_PROGRESS and SUCCESSFUL. As part of the recent blue/green release, there is a new subcategory underneath these states, known as lifecycle stages. When a deployment has started, we work through each lifecycle stage linearly, with the option to extend the deployment with custom logic packaged in lifecycle hooks. The following figure shows the relationship between lifecycle states, lifecycle stages, and lifecycle hooks.\nLifecycle hooks are synchronous AWS Lambda functions that the Amazon ECS control plane invokes on your behalf. You can write any logic you’d like in these functions in any runtime language you choose. When the function has finished executing its logic, it must return a hookStatus for the Amazon ECS deployment to continue. If a hookStatus is not returned, or if the function fails, then the Amazon ECS deployment rolls back.\nThe values of the hookStatus can either be:\nSUCCEEDED: the deployment continues to the next lifecycle stage. FAILED: the deployment rolls back to the last successful deployment. IN_PROGRESS: the Amazon ECS control plane invokes the function again after a short period of time. By default, this is 30 seconds, but this can be tuned by returning a callBackDelay. IN_PROGRESS hook status The IN_PROGRESS status provides a powerful and flexible mechanism to monitor and control the Amazon ECS deployment. The function is a synchronous event, thus the deployment can only pause for the maximum execution time of a function (15 minutes). There may be scenarios where you need to execute complex logic or pause for more data, such as waiting for a full test suite to run against the test endpoint. This hookStatus response allows you to achieve this. When the IN_PROGRESS hook status is returned, the Amazon ECS control plane will invoke the function again in 30 seconds (or the time returned in the callBackDelay key). Amazon ECS will continue to invoke the function until a SUCCEEDED or FAILED hook status is returned; therefore, allowing for validation logic that needs more time than the execution time of a single function.\nPassing state with hookDetails Lifecycle hooks are often defined once and reused across multiple ECS services within an organization. To make lifecycle hooks ECS service agnostic, you can use the hookDetails dictionary to pass in service specific data when creating or update a service. Below is an extract from a ECS service where we have defined a lifecycle hook and passed in an Amazon S3 Bucket name.\n{ \u0026#34;hookTargetArn\u0026#34;: \u0026#34;arn:aws:lambda:\u0026lt;region\u0026gt;:\u0026lt;account_id\u0026gt;:function:\u0026lt;function_name\u0026gt;\u0026#34;, \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;account_id\u0026gt;:role/\u0026lt;iam_role_name\u0026gt;\u0026#34;, \u0026#34;lifecycleStages\u0026#34;: [\u0026#34;POST_TEST_TRAFFIC_SHIFT\u0026#34;], \u0026#34;hookDetails\u0026#34;: { \u0026#34;S3_BUCKET_NAME\u0026#34;: \u0026#34;mys3bucket\u0026#34; }, } Furthermore, you can also use the hookDetails dictionary to pass state between invocations of the same lifecycle hook. If a hook returns the IN_PROGRESS status, you can also include the hookDetails dictionary with multiple key / value pairs, removing the need to store state elsewhere. Common use cases include passing metric counters or Amazon Resource Names (ARNs) of external resources between invocations. Note data added to hookDetails in this way only persists between invocations of the same hook within a single deployment. Data doesn’t carry over between different hooks within the same deployment or the same hook in different deployments.\n{ \u0026#34;hookStatus\u0026#34;: \u0026#34; IN_PROGRESS\u0026#34;, \u0026#34;callBackDelay\u0026#34;: 90, \u0026#34;hookDetails\u0026#34;: { \u0026#34;SFN_EXECUTION_ARN\u0026#34;: \u0026#34;arn:aws:states:\u0026lt;region\u0026gt;:\u0026lt;account_id\u0026gt;:execution:\u0026lt;sfn\u0026gt;:\u0026lt;id\u0026gt;\u0026#34;, }, } PRE_SCALE_UP hook The PRE_SCALE_UP lifecycle hook is unique in Amazon ECS deployments as it is the only hook available for both the new blue green deployment strategy as well as the existing rolling update strategy. The PRE_SCALE_UP hook runs before new tasks are created, allowing you to apply admission logic in Amazon ECS. A common use of this hook is to evaluate the Amazon ECS task definition against a governance policy before the tasks have been scheduled. Previously, if you wanted to enforce a policy to tasks, then you could only respond to a task once it has been scheduled. An example of the previous pattern can be found in this GitHub repository. This previous pattern had limitations where insecure or vulnerable tasks could be running before your policy detected an infringement. This new hook enables use cases such as verifying a container image signature or a container image repository before the workload has been scheduled.\nWalkthrough In this walkthrough we are going to show different patterns to demonstrate the power of lifecycle hooks. We will first showcase how a PRE_SCALE_UP hook can be used as form of admission hook, secondly, we will show a POST_TEST_TRAFFIC_SHIFT hook can be used to apply a manual approval step before production traffic is shifted.\nPrerequisites To deploy this walk through you need access to an AWS account with a workstation with the AWS Command Line Interface (AWS CLI), AWS Cloud Development Kit (AWS CDK), and Docker Engine installed.\nThroughout this walkthrough we deploy resources on to an Amazon ECS cluster and into an Amazon Virtual Private Cloud (Amazon VPC). The code for this walk through can be found in an AWS CDK app within our sample GitHub repository.\nClone down the GitHub Repository $ git clone https://github.com/aws-samples/sample-amazon-ecs-blue-green-deployment-patterns.git Deploy the AWS CDK app. This AWS CDK app contains three separate AWS CloudFormation stacks. It creates an Amazon VPC and Application Load Balancer (ALB) in the first stack. In the second, it creates an ECS cluster and necessary AWS Identity and Access Management (IAM) roles. Third, it creates a stack containing our functions. $ cd sample-amazon-ecs-blue-green-deployment-patterns/ecs-bluegreen-lifecycle-hooks $ npm install $ npm run build $ npx cdk deploy --all You can verify that the resources were created correctly with the following:\n$ aws cloudformation list-stacks \\ --query \u0026#34;StackSummaries[?starts_with(StackName, \u0026#39;Ecs\u0026#39;)].[StackName, StackStatus]\u0026#34; \\ --output table Within this repository we have a template task definition and service definition. We have created a populate templates bash script within the repository to populate these templates with the values relevant to your environment. $ ./populate_templates.sh Admission hook demonstration To demonstrate how a PRE_SCALE_UP hook can be used as an admission hook, we are going to start an Amazon ECS service using a container image from an Amazon Elastic Container Registry (Amazon ECR) public repository. In our scenario, we have written a policy in our function to make sure that all container images come from either a private or public Amazon ECR repository. We can see this policy being enforced in the logs of our lifecycle hook.\nFirst, we look at the resources we plan to deploy: Inside of our service definition you should find a PRE_SCALE_UP lifecycle hook. $ cat outputs/service.json | jq -r \u0026#39;.deploymentConfiguration.lifecycleHooks[0]\u0026#39; { \u0026#34;hookTargetArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:111222333444:function:EcsBluegreenHookStack-admissionFunction1D626CB0-FepCuhdYPhNn\u0026#34;, \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::111222333444:role/EcsBluegreenEcsStack-ECSLambdaInvokeRole2A82A552-yEE1nb8PewjH\u0026#34;, \u0026#34;lifecycleStages\u0026#34;: [ \u0026#34;PRE_SCALE_UP\u0026#34; ] } The function for this hook verifies our container image using a regex expression. You can see that piece of code locally by printing the source code.\n$ grep -A 20 \u0026#34;def validate_container_images\u0026#34; src/admissionFunction/app.py def validate_container_images(container_image_list): # Regex patterns to match Amazon ECR repository URLs # Format for private ECR: [account-id].dkr.ecr.[region].amazonaws.com/[repository-name]:[tag] # Format for public ECR: public.ecr.aws/aws-containers/[repository-name]:[tag] private_ecr_pattern = r\u0026#34;^(\\d+)\\.dkr\\.ecr\\.([a-z0-9-]+)\\.amazonaws\\.com/.*\u0026#34; public_ecr_pattern = r\u0026#34;^public\\.ecr\\.aws/aws-containers/.*\u0026#34; valid_images = True for container_image in container_image_list:# image_url = container_image[\u0026#34;image\u0026#34;] logger.info(f\u0026#34;Validating image: {image_url}\u0026#34;) if re.match(private_ecr_pattern, image_url) or re.match( public_ecr_pattern, image_url ): logger.info(f\u0026#34;Image {image_url} is from an Amazon ECR repository\u0026#34;) else: logger.warning(f\u0026#34;Image {image_url} is NOT from an Amazon ECR repository\u0026#34;) valid_images = False return valid_images Finally, you can see what container image we are attempting to deploy within our task definition.\n$ cat outputs/taskdef.json | jq -r \u0026#39;.containerDefinitions[0].image\u0026#39; public.ecr.aws/aws-containers/retail-store-sample-ui:1.1.0 We create the Amazon ECS service and see the lifecycle hook in action. aws ecs \\ register-task-definition \\ --cli-input-json file://outputs/taskdef.json aws ecs \\ create-service \\ --cli-input-json file://outputs/service.json In the Amazon console you can monitor this deployment. The** Current Deployment stage** within the Deployment tab shows us which stage of the lifecycle the deployment is in. Remember our first lifecycle hook runs at the PRE_SCALE_UP stage. You can verify that the lifecycle hook executed successfully by browsing to the Amazon CloudWatch Logs console, and choosing the Log Group starting with the name /aws/lambda/EcsBlugreenHookStack-admissionFunction. Success! The container images used within our task definition were verified against our policy before the task was scheduled.\nManual approval demonstration The second part of our walkthrough focuses on a manual approval step at the POST_TEST_TRAFFIC_SHIFT stage. This is a critical stage in our deployment pipeline, because it’s after the test traffic endpoint has been shifted to our green tasks (allowing you to a run a test suite), but before the production traffic endpoint has been moved.\nIn this walkthrough we implement a manual approval step before migrating the production traffic. There are lots of ways to implement a manual approval step, but in this demonstration, we use a file and an Amazon S3 bucket. The function in our lifecycle hook attempts to find a file in an S3 bucket named after the Amazon ECS service revision. If the file exists, then the deployment pipeline continues. If the file does not exist, then the hook returns an IN_PROGRESS state and we try again in 30 seconds.\nFirst, we look at the resources that we plan to deploy. Inside of our service definition we should find a POST_TEST_TRAFFIC_SHIFT lifecycle hook. $ cat outputs/service.json | jq -r \u0026#39;.deploymentConfiguration.lifecycleHooks[1]\u0026#39; { \u0026#34;hookTargetArn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:111222333444:function:EcsBluegreenHookStack-approvalFunctionC7093965-J3YoEQtxR679\u0026#34;, \u0026#34;roleArn\u0026#34;: \u0026#34;arn:aws:iam::111222333444:role/EcsBluegreenEcsStack-ECSLambdaInvokeRole2A82A552-9OcKcmXkLe9c\u0026#34;, \u0026#34;lifecycleStages\u0026#34;: [ \u0026#34;POST_TEST_TRAFFIC_SHIFT\u0026#34; ] } Browsing the function source code locally allows us to look at the logic to find the Amazon S3 file. We are using the boto3client to attempt to find the head object of a file named after our Amazon ECS service revision ID.\n$ grep -A 20 \u0026#34;def check_s3_file\u0026#34; src/approvalFunction/app.py def check_s3_file(s3_bucket, revision_arn): # Extract the revision from the ARN (everything after the last \u0026#39;/\u0026#39;) revision = revision_arn.split(\u0026#34;/\u0026#34;)[-1] file_name = f\u0026#34;{revision}.txt\u0026#34; logger.info(f\u0026#34;Checking if file {file_name} exists in bucket {s3_bucket}\u0026#34;) # Create S3 client s3_client = boto3.client(\u0026#34;s3\u0026#34;) try: # Use head_object to check if the file exists s3_client.head_object(Bucket=s3_bucket, Key=file_name) logger.info(f\u0026#34;File {file_name} exists in bucket {s3_bucket}\u0026#34;) return True except ClientError as e: if e.response[\u0026#34;Error\u0026#34;][\u0026#34;Code\u0026#34;] == \u0026#34;404\u0026#34;: # The file does not exist logger.info(f\u0026#34;File {file_name} does not exist in bucket {s3_bucket}\u0026#34;) return False To trigger a deployment, we update our Amazon ECS service with a force action. This forces Amazon ECS to create a new service revision and redeploy all the tasks, even though the service specification hasn’t changed. export CLUSTER_NAME=$(cat outputs/service.json | jq -r \u0026#39;.cluster\u0026#39;) export SERVICE_NAME=$(cat outputs/service.json | jq -r \u0026#39;.serviceName\u0026#39;) aws ecs \\ update-service \\ --service $SERVICE_NAME \\ --cluster $CLUSTER_NAME \\ --force You can monitor the ongoing deployment in the Amazon ECS console, watching the service move through the various lifecycle stages. When the deployment reaches the POST_TEST_TRAFFIC_SHIFT stage, we can check the progress of our hook. The CloudWatch Logs console gives us insight into what is happening at this stage. In a log group starting with /aws/lambda/EcsBlugreenHookStack-approvalFunction, we should see invocations of our function every 30 seconds attempting to retrieving the file from Amazon S3. To allow our deployment pipeline to continue, we upload a file to the S3 bucket. First, we need to retrieve the Amazon ECS service revision ARN, shorten it to just the service revision ID, and use that as the file name. export S3_BUCKET_NAME=$(aws cloudformation describe-stacks \\ --stack-name EcsBluegreenHookStack \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ApprovalBucketName`].OutputValue\u0026#39; \\ --output text) export ECS_SERVICE_REVISION=$(aws ecs \\ list-service-deployments \\ --service $SERVICE_NAME \\ --cluster $CLUSTER_NAME \\ --query \u0026#39;serviceDeployments[?status==`IN_PROGRESS`].targetServiceRevisionArn\u0026#39; \\ --output text) SERVICE_REVISION_ID=$(echo \u0026#34;$ECS_SERVICE_REVISION\u0026#34; | awk -F/ \u0026#39;{print $NF}\u0026#39;) touch $SERVICE_REVISION_ID.txt aws s3 cp $SERVICE_REVISION_ID.txt s3://$S3_BUCKET_NAME/$SERVICE_REVISION_ID.txt Now that the file exists in Amazon S3, we can go back to the Amazon ECS console and watch the deployment complete. Cleaning up To clean up the AWS resources used throughout this walkthrough, we must remove the Amazon ECS service.\n$ aws ecs \\ delete-service \\ --service $SERVICE_NAME \\ --cluster $CLUSTER_NAME \\ --force Next, we delete the CloudFormation stacks using AWS CDK.\n$ cdk destroy --all Conclusion Lifecycle hooks in Amazon ECS provide endless possibilities to customize your deployment pipeline. In this post we have covered a few common use cases for these hooks, such as evaluating a deployment against governance policies, as well as implementing a manual approval step. To learn more, go to the Amazon ECS lifecycle hooks documentation, and watch demonstrations on the Containers from the Couch YouTube channel.\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Accelerate serverless testing with LocalStack integration in VS Code IDE by Micah Walter | on 11 SEP 2025 | in Announcements, AWS Lambda, AWS Serverless Application Model, Developer Tools, Launch, News, Serverless | Permalink | Comments\nToday, we’re announcing LocalStack integration in the AWS Toolkit for Visual Studio Code that makes it easier than ever for developers to test and debug serverless applications locally. This enhancement builds upon our recent improvements to the AWS Lambda development experience, including the console to IDE integration and remote debugging capabilities we launched in July 2025, continuing our commitment to simplify serverless development on Amazon Web Services (AWS).\nWhen building serverless applications, developers typically focus on three key areas to streamline their testing experience: unit testing, integration testing, and debugging resources running in the cloud. Although AWS Serverless Application Model Command Line Interface (AWS SAM CLI) provides excellent local unit testing capabilities for individual Lambda functions, developers working with event-driven architectures that involve multiple AWS services, such as Amazon Simple Queue Service (Amazon SQS), Amazon EventBridge, and Amazon DynamoDB, need a comprehensive solution for local integration testing. Although LocalStack provided local emulation of AWS services, developers had to previously manage it as a standalone tool, requiring complex configuration and frequent context switching between multiple interfaces, which slowed down the development cycle.\nLocalStack integration in AWS Toolkit for VS Code To address these challenges, we’re introducing LocalStack integration so developers can connect AWS Toolkit for VS Code directly to LocalStack endpoints. With this integration, developers can test and debug serverless applications without switching between tools or managing complex LocalStack setups. Developers can now emulate end-to-end event-driven workflows involving services such as Lambda, Amazon SQS, and EventBridge locally, without needing to manage multiple tools, perform complex endpoint configurations, or deal with service boundary issues that previously required connecting to cloud resources.\nThe key benefit of this integration is that AWS Toolkit for VS Code can now connect to custom endpoints such as LocalStack, something that wasn’t possible before. Previously, to point AWS Toolkit for VS Code to their LocalStack environment, developers had to perform manual configuration and context switching between tools.\nGetting started with LocalStack in VS Code is straightforward. Developers can begin with the LocalStack Free version, which provides local emulation for core AWS services ideal for early-stage development and testing. Using the guided application walkthrough in VS Code, developers can install LocalStack directly from the toolkit interface, which automatically installs the LocalStack extension and guides them through the setup process. When it’s configured, developers can deploy serverless applications directly to the emulated environment and test their functions locally, all without leaving their IDE.\nLet’s try it out First, I’ll update my copy of the AWS Toolkit for VS Code to the latest version. Once, I’ve done this, I can see a new option when I go to Application Builder and click on Walkthrough of Application Builder. This allows me to install LocalStack with a single click.\nOnce I’ve completed the setup for LocalStack, I can start it up from the status bar and then I’ll be able to select LocalStack from the list of my configured AWS profiles. In this illustration, I am using Application Composer to build a simple serverless architecture using Amazon API Gateway, Lambda, and DynamoDB. Normally, I’d deploy this to AWS using AWS SAM. In this case, I’m going to use the same AWS SAM command to deploy my stack locally.\nI just do sam deploy –guided –profile localstack from the command line and follow the usual prompts. Deploying to LocalStack using AWS SAM CLI provides the exact same experience I’m used to when deploying to AWS. In the screenshot below, I can see the standard output from AWS SAM, as well as my new LocalStack resources listed in the AWS Toolkit Explorer.\nI can even go in to a Lambda function and edit the function code I’ve deployed locally!\nOver on the LocalStack website, I can login and take a look at all the resources I have running locally. In the screenshot below, you can see the local DynamoDB table I just deployed.\nEnhanced development workflow These new capabilities complement our recently launched console-to-IDE integration and remote debugging features, creating a comprehensive development experience that addresses different testing needs throughout the development lifecycle. AWS SAM CLI provides excellent local testing for individual Lambda functions, handling unit testing scenarios effectively. For integration testing, the LocalStack integration enables testing of multiservice workflows locally without the complexity of AWS Identity and Access Management (IAM) permissions, Amazon Virtual Private Cloud (Amazon VPC) configurations, or service boundary issues that can slow down development velocity.\nWhen developers need to test using AWS services in development environments, they can use our remote debugging capabilities, which provide full access to Amazon VPC resources and IAM roles. This tiered approach frees up developers to focus on business logic during early development phases using LocalStack, then seamlessly transition to cloud-based testing when they need to validate against AWS service behaviors and configurations. The integration eliminates the need to switch between multiple tools and environments, so developers can identify and fix issues faster while maintaining the flexibility to choose the right testing approach for their specific needs.\nNow available You can start using these new features through the AWS Toolkit for VS Code by updating to v3.74.0. The LocalStack integration is available in all commercial AWS Regions except AWS GovCloud (US) Regions. To learn more, visit the AWS Toolkit for VS Code and Lambda documentation.\nFor developers who need broader service coverage or advanced capabilities, LocalStack offers additional tiers with expanded features. There are no additional costs from AWS for using this integration.\nThese enhancements represent another significant step forward in our ongoing commitment to simplifying the serverless development experience. Over the past year, we’ve focused on making VS Code the tool of choice for serverless developers, and this LocalStack integration continues that journey by providing tools for developers to build and test serverless applications more efficiently than ever before.\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Build centralized cross-Region backup architecture with AWS Control Tower by Chris Falk, Lei Shi, và Pujah Goviel | on 11 SEP 2025 | in Advanced (300), AWS Backup, AWS Control Tower, AWS Key Management Service (KMS), AWS Organizations, Best Practices, Enterprise governance and control, Resilience, Storage | Permalink | Comments\nManaging data protection at scale is a critical challenge for the modern enterprise. As organizations grow, their data becomes increasingly distributed, making it difficult to implement consistent backup policies that ensure comprehensive coverage. IT teams must balance competing needs of compliance requirements, resource protection, and operational efficiency – all while struggling to validate and orchestrate backup procedures across an expanding digital footprint.\nAWS Backup offers a powerful solution to these challenges with a centralized, fully-managed service that streamlines data protection at scale. Enterprises can leverage AWS Organizations alongside AWS Backup to implement automated, consistent backup policies across their entire cloud environment. The integration with AWS Control Tower further simplifies this process by enabling organizations to incorporate enterprise-wide backup management directly into their well-architected multi-account landing zone.\nIn this post, we’ll demonstrate how to implement AWS Backup using AWS Control Tower integration. We’ll explore the architecture, prerequisites, and step-by-step implementation process. By following this guide, you’ll learn how to automatically deploy and manage backup policies across your organization, helping to meet compliance requirements, protect critical resources, and reduce administrative overhead. This solution is particularly valuable to enterprises looking to standardize backup operations while scaling their cloud environment.\nSolution overview AWS Control Tower now offers built-in capabilities to streamline your backup management at scale with direct AWS Backup integration. This capability automatically provisions a central backup vault in each AWS Region within a dedicated central backup account. As your organization scales to more accounts and regions, AWS Control Tower automatically creates local backup vaults in each workload account in AWS Backup-enabled organizational units (OUs) across all governed AWS Regions. You can customize your backup strategy by configuring backup policies that copy your backups from local vaults to central vaults, either within the same Region or cross-Region. Moreover, you don’t need custom deployment pipelines or complex automation. You can use AWS Backup policies to implement a comprehensive backup strategy that meets your organization’s specific requirements while maintaining consistent governance across your AWS environment.\nPrerequisites Before implementing the AWS Control Tower and AWS Backup integration, make sure that you have the following foundational elements in place:\nAn existing AWS Organization governed by AWS Control Tower Administrative access to the AWS Organizations management account Walkthrough This section walks you through the solution steps.\nStep 1: Provision dedicated backup accounts First, you must create two specialized accounts that form the backbone of your backup infrastructure:\nBackup administrator account: This account manages backup policies and configurations for your entire organization. Central backup account: This account stores centralized backup vaults and cross-account backup copies. Create these two accounts through the Organizations console or AWS Command Line Interface (AWS CLI). Do not use AWS Control Tower Account Factory or any other AWS account vending process that enrolls new accounts in AWS Control Tower governance. In step three, when you enable AWS Backup in AWS Control Tower, the accounts are enrolled and placed into the Security OU automatically.\nStep 2: Create a multi-Region AWS KMS Key Backup encryption is a crucial security requirement in enterprise organizations. Create a multi-Region AWS Key Management Service (AWS KMS) key in your management account to enable encryption of backup data at rest and secure cross-account backup capabilities. The key should be created in your AWS Control Tower Home Region and replicated to all governed AWS Regions. This multi-Region AWS KMS key is used to encrypt backups in the central backup account vaults, and all local vaults in the workload accounts in all Regions. Although you can create the key manually through the AWS Management Console, we recommend infrastructure as code (IaC) (AWS CloudFormation, AWS Cloud Development Kit (AWS CDK), or Terraform).\nRefer to the AWS Control Tower documentation for more details on how to create the relevant AWS KMS key policy.\nStep 3: Enable AWS Backup in AWS Control Tower You can now to integrate AWS Backup with your AWS Control Tower landing zone. Navigate to AWS Control Tower console, choose Landing zone settings, and choose Update landing zone. In the backup configuration section, type or choose the following details:\nCentral backup account ID: The 12-digit account ID of the Central Backup account that you created in Step 1. Backup administrator account ID: The 12-digit account ID of the backup administrator account that you created in Step 1. KMS Key ARN: The Amazon Resource Name (ARN) of the AWS KMS key that you created in Step 2. You can choose the accounts created in Step 1 and the AWS KMS key in Step 2 from the drop down list in each text box, as shown in the following figure.\nThe landing zone update process may take 30-45 minutes to complete. When it is finished, you should see the AWS Backup status as Enabled, and your specified accounts and AWS KMS keys should appear in the AWS Control Tower console, as shown in the following figure.\nStep 4: Configure service opt-in and enable delegated administrator After enabling AWS Backup in AWS Control Tower, configure service opt-in in the AWS Backup console.\nNavigate to AWS Backup console, choose Settings, and in the Service opt-in list:\nEnable opt-in for services needed for your workloads that you would like to cover with AWS Backup, such as Amazon Relational Database Service (Amazon RDS), Amazon Elastic Compute Cloud (Amazon EC2), Amazon DynamoDB, Amazon Elastic Block Store (Amazon EBS), Amazon Elastic File System (Amazon EFS). This must be done manually in the AWS Backup console. Backup service opt-in is an AWS Region-based setting, and you must repeat the process for all AWS Control Tower governed AWS Regions. Finally, set the backup administrator account as a delegated administrator to enable organization-wide backup task monitoring and backup policy management from this account, as shown in the following figure.\nStep 5: Enable AWS Backup baseline on specific OUs After configuring the preceding core backup infrastructure, you must enable the AWS Backup baseline on your desired OUs. Navigate to the AWS Control Tower console, choose Organization from the left navigation pane, choose the OU where you want to enable backup, then find and enable the option AWS Backup on this OU, as shown in the following figure\nThe order of enablement is critical, so when enabling the AWS Backup baseline across your organization, follow a hierarchical approach. Start with the top-level OUs before proceeding to child OUs. In the example shown in the following figure, you would first enable the AWS Backup baseline on the Workloads top-level OU, then proceed to child OUs such as Workloads X, Workloads Y, etc.\nStep 6: Tag resources for backup The final step in your implementation is to apply tags to the resources to be backed up.\nWhen you enable AWS Backup through AWS Control Tower, a set of default backup plans with pre-defined resource tags are automatically created for hourly, daily, weekly, and monthly backups. These plans provide reasonable protection for common scenarios. However, most enterprises will want to implement custom backup plans based on workload criticality and compliance requirements by using AWS Organizations backup policies.\nCleaning up The following steps walk you through cleaning up after completion of this solution.\nStep 1: Disable backup baseline on specific OUs Before removing the integration at the landing zone level, you should first disable the backup controls at each OU.\nNavigate to the AWS Control Tower console Choose Organization from the left navigation pane Following the reverse order of your enablement process, starting with child OUs before proceeding to parent OUs:: a. Choose the OU b. Choose the Controls tab c. Find and disable the AWS Backup on this OU option Step 2: Remove the AWS Backup integration Now you are ready to remove the AWS Backup integration from your AWS Control Tower landing zone.\nNavigate to the AWS Control Tower console Choose Landing zone settings Choose Update landing zone In the backup configuration section, choose AWS Backup is not enabled Choose Next, review the changes, then choose Update landing zone to apply these changes AWS Control Tower removes the integration while preserving your existing backup data, as shown in the following figure.\nStep 3: Clean up additional resources After the integration is removed, you may want to clean up more resources:\nSchedule deletion for the AWS KMS key that was created Remove any backup-specific AWS Identity and Access Management (IAM) roles that are no longer needed Decide whether to repurpose or close the dedicated backup accounts Delete any recovery points and vaults that are no longer needed By default, removing the AWS Backup integration does not delete your existing backup vaults or recovery points. This is by design to prevent accidental data loss during configuration changes. However, this means that you must manage these resources explicitly if you want to remove them. To manage your backup vaults and recovery points after removing the integration, navigate to the AWS Backup console in your central backup account, and choose Backup vaults from the left navigation pane. To delete the entire vault and all recovery points, first delete all recovery points within the vault, then choose the vault and choose .\nConclusion By integrating AWS Backup with AWS Control Tower, you can automate and standardize data protection across your entire enterprise without custom development. Organizations using AWS Control Tower can enable AWS Backup as a baseline landing zone service to automate backup vault creation and policy deployment across multiple accounts and AWS Regions, ensuring consistent data protection while reducing administrative overhead. The centralized management approach streamlines operations, while flexible policy options allow you to tailor protection strategies to your specific business needs. Most importantly, this solution enhances overall data resilience through built-in best practices. As your cloud environment scales, you can confidently maintain robust, consistent backup practices that align with well-architected principles.\nTAGS: AWS Backup, AWS Cloud Storage, AWS Control Tower, AWS Key Management Service (AWS KMS), data resiliency "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders.” Event Objectives Panel discussion: Navigating the GenAI Revolution: Strategies for Executive Leadership. Track 1: GenAI And Data. Building a Unified Data Foundation on AWS for AI and Analytics Workloads. Building the Future: Gen AI Adoption and Roadmap on AWS. AI-Driven Development Lifecycle (AI-DLC) Shaping the future of Software Implementation. Securing Generative AI Applications with AWS: Fundamentals and Best Practices. Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers. Speakers Jun Kai Loke – AI/ML Specialist SA, AWS. Kien Nguyen – Solutions Architect, AWS. Tamelly Lim – Storage Specialist SA, AWS. Binh Tran – Senior Solutions Architect, AWS. Taiki Dang – Solutions Architect, AWS. Michael Armentano – Principal WW GTM Specialist, AWS. Key Highlights Panel discussion: Navigating the GenAI Revolution: Strategies for Executive Leadership. This discussion explores how executive leaders can guide their organizations through generative AI advancements, with panelists sharing insights on fostering innovation, aligning AI with business goals, and managing organizational change during AI adoption.\nTrack 1: GenAI And Data. Building a Unified Data Foundation on AWS for AI and Analytics Workloads. This session explores strategies and best practices for building a unified, scalable data foundation on AWS to support AI and analytics workloads. It covers how to use AWS services for data ingestion, storage, processing, and governance, enabling organizations to manage and leverage data effectively for advanced analytics and AI applications.\nBuilding the Future: Gen AI Adoption and Roadmap on AWS. This session will present AWS’s overall vision, emerging trends, and strategic roadmap for adopting Generative AI (GenAI). It will focus on key AWS services and initiatives that help organizations leverage GenAI to drive innovation and efficiency.\nAI-Driven Development Lifecycle (AI-DLC) Shaping the future of Software Implementation. The AI-Driven Development Lifecycle (AI-DLC) is an AI-centric approach that embeds AI throughout the software development process. Unlike traditional methods where AI is secondary, AI-DLC integrates AI execution with human oversight and collaboration to significantly enhance development speed, quality, and innovation.\nSecuring Generative AI Applications with AWS: Fundamentals and Best Practices. This session examines security challenges across the generative AI stack—covering infrastructure, models, and applications—and explains how AWS uses measures like encryption, zero-trust architecture, monitoring, and access controls to protect AI workloads and ensure data confidentiality and integrity.\nBeyond Automation: AI Agents as Your Ultimate Productivity Multipliers. This session highlights how AI agents can act as intelligent partners, autonomously learning, adapting, and executing complex tasks to transform operations, greatly enhancing efficiency and productivity.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Hoang Van Anh\nPhone Number: 0375956650\nEmail: anhhvse180142@fpt.edu.vn\nStudent ID: SE180142\nUniversity: FPT University Camp Ho Chi Minh City\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/09/2025 to 09/02/2026\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Went to the office with the team and learned the office rules. Learned how to write a workshop. Explored AWS and created an account. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Clearly understood the office rules and will follow them in future sessions. 08/09/2025 09/09/2025 2 -Learned how to draft the structure of a workshop. 10/09/2025 13/09/2025 https://www.youtube.com/watch?v=mXRqgMr_97U\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=3 3 - Created an AWS account and completed the tasks to receive the free $200 credit. - Practiced basic tasks related to the $200 credit. 14/09/2025 14/09/2025 4 - Studied and understood the concept of cloud computing. 15/09/2025 17/09/2025 https://www.youtube.com/watch?v=HxYZAK1coOI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=4 5 - Perform creating an IAM user to manage the admin user and admin group. 16/09/2025 16/09/2025 https://www.youtube.com/watch?v=HxYZAK1coOI\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=4 Week 1 Achievements: Understanding Cloud Computing:\nCloud computing is the delivery of IT resources over the Internet. Pay-as-you-go model, costs based on usage. Cost optimization: only pay for what you use, with high speed. Flexible in adding or reducing resources as needed. Global scale, widely applicable across the world. How to learn Cloud Computing:\nLearn and share knowledge with peers, and gain experience from those with more expertise. Practice by using AWS services. Learn through self-study on online platforms. Creating an AWS account and understanding AWS tools:\nSuccessfully created an account and received $200 after completing activation tasks. Gained a clear understanding of the difference between Root User and IAM User: Root User is the main account that manages everything. IAM Users are created by the Root User and assigned permissions. Set up MFA for the account to ensure security. Create user:\nTo create a user, go to the AWS console and search for IAM (Identity and Access Management). Then you’ll see its Dashboard; pay attention to two sections: Users and User Groups. To create a user, select Users and click create user. Enter the username and check Provide user access to the AWS Management Console - optional. To create a password, select I want to create an IAM user =\u0026gt; I want to create an IAM user =\u0026gt; Next. Click Next, then finally Create user. Create user:\nTo create a User Group, go to the AWS console and search for IAM (Identity and Access Management). Select User Groups =\u0026gt; Create user group =\u0026gt; Enter a group name and create it. Open the group you just created, choose Add user, select the user you want to add, and then add the user. "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Study and understand what VPC and VPN are How to create, connect, and operate networking services Experimentation and results Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn the concepts of VPC and VPN 08/11/2025 08/11/2025 2 - Practice: Use AWS services with VPC and VPN 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 3 - Create Subnets, Internet Gateway 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create Route Table, security groups 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Create Route Table, security groups 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understanding VPC and VPN:\nAmazon VPC (Virtual Private Cloud): Allows you to create a private virtual network to launch AWS resources such as virtual servers and virtual networks, with dedicated management. A VPC can span multiple AZs (Availability Zones) but exists in only one Region. When creating and configuring, you must specify an IPv4 CIDR block (mandatory), while IPv6 is optional. Maximum: 5 VPCs per Region per Account. Purpose: Used to separate environments (Production, Dev, Test, Staging). You can divide instances into subnets, each subnet must belong to a specific AZ and be assigned a CIDR block that is a subset of the VPC’s CIDR. Each subnet reserves 5 IP addresses: Network address (e.g., 10.10.1.0) Broadcast addres (e.g., 10.10.1.255) Router address (10.10.1.1) DNS address (10.10.1.2) Future use (10.10.1.3) Route Table:\nA set of routes that determine the path for network traffic. Default Route Table: Cannot be deleted, contains a single route that allows subnets in the VPC to communicate with each other. Each subnet must be associated with a route table. Custom Route Table: Can be created, but the default route cannot be removed. ENI (Elastic Network Interface): A virtual network card attached to EC2 instances.\nEnsures network settings remain consistent even if the server is replaced. Private IP. Elastic IP. MAC address. VPC Endpoint: Allows private connection to AWS services within the VPC without using the internet.\nTwo types: Interface Endpoint and Gateway Endpoint. VPC Security Group: A stateful virtual firewall that controls inbound and outbound traffic to AWS resources.\nSecurity Group rules are limited to source IP, port, and other Security Groups. Only allow rules are supported, applied to ENIs. NACL (Network Access Control List): A stateless virtual firewall that controls inbound and outbound traffic at the subnet level.\nRules limited by source IP and port. Applied to VPC Subnets. Default NACL allows all inbound and outbound traffic. VPC Flow Logs: Capture information about IP traffic going into and out of the VPC.\nLogs are stored in Amazon CloudWatch Logs or S3. Does not capture packet contents. VPC Peering: Enables direct connectivity between two VPCs, requires 1:1 peering connections, does not support overlapping IP address ranges.\nTransit Gateway: A hub for interconnecting multiple VPCs.\nVPN:\nSite-to-Site VPN: For hybrid models, providing continuous connectivity between on-premises and VPC.\nClient-to-Site VPN: Allows individual hosts to securely connect to VPC resources.\nAWS Direct Connect: Provides dedicated private connection between on-premises and AWS with latency of 20–30ms.\nElastic Load Balancing (ELB): A managed load balancing service.\nSupports HTTP, HTTPS, TCP (secure), SSL.\nCan be public or private.\nFour types: Application Load Balancer (ALB), Network Load Balancer (NLB), Classic Load Balancer (CLB), Gateway Load Balancer (GLB)\nPractice:\nSearch for VPC → create VPC → give it a name Choose IPv4 and enter the address 10.10.0.0/16 Create Subnets Create Internet gateways Create Security groups Create Security groups "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Get familiar with EC2: how to create and connect to an EC2 instance. Set up Hybrid DNS with a Route Table and connect EC2 with the Endpoint. Run the EC2 instance and verify that the environment is working. Delete the resources after finishing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Create an EC2 instance with Subnets, verify the connection, and create a NAT Gateway. 21/09/2025 22/09/2025 2 - Connect EC2 with the Endpoint, create a key pair and DNS. 24/09/2025 25/09/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 3 - Create Route 53: Resolver Rules, Inbound Endpoints. 27/09/2025 28/09/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Verify the results and delete the resources. 28/09/2025 28/09/2025 https://www.youtube.com/watch?v=AQlsd0nWdZk\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i Week 3 Achievements: Create an EC2 instance: first connect on AWS, then connect using PuTTY to access the CLI, and enter the user login to sign in. "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn about virtual machines on AWS. What operating systems are available and which ones are the most commonly used on AWS. How EC2 is managed and why it is widely used. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Amazon Elastic Compute Cloud ( EC2 ) - Instance type AWS. 29/09/2025 29/09/2025 2 - Amazon Elastic Compute Cloud ( EC2 ) - AMI / Backup / Key Pair EC2. 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Amazon Elastic Compute Cloud ( EC2 ) - Elastic block store. 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Amazon Elastic Compute Cloud ( EC2 ) - Instance store 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Amazon Elastic Compute Cloud ( EC2 ) - User data 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Amazon Elastic Compute Cloud ( EC2 ) - Meta data 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 7 - Amazon Elastic Compute Cloud ( EC2 ) - EC2 auto scaling 1/10/2025 1/10/2025 https://cloudjourney.awsstudygroup.com/ 8 - EC2 Autoscaling - EFS/FSx - Lightsail - MGN 1/10/2025 1/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Concept of virtual machines and virtual machine services:\nAmazon Elastic Compute Cloud (EC2) Amazon Lightsail Amazon EFS/FSx AWS Application Migration Service (MGN) Amazon Elastic Compute Cloud (EC2):\nEC2 is a virtual machine service, similar to physical servers and traditional virtual servers. However, EC2 can be launched faster and provides more flexible resource scalability.\nEC2 can support workloads for hosting websites, applications, databases, and essentially anything that traditional servers can do, EC2 can also handle.\nTo be considered an Instance type, it must include the following factors.\nCPU/GPU. Memory. Network. Storage AMI(Amazon Machine Image)/Backup/Keypair.\nAMI (Amazon Machine Image) is used to provision one or multiple EC2 Instances at the same time. AMIs are available from AWS, on the AWS Marketplace, or as custom AMIs created from EC2 Instances. An AMI includes root OS volumes, usage permissions that define which AWS accounts can use the AMI, and EBS volume mappings that will be created and attached to EC2 Instances. EC2 Instances can be backed up by creating snapshots. Key pairs (public key and private key) are used to encrypt login information for EC2 Instances. Elastic block store.\nAmazon EBS provides block storage and is directly attached to an EC2 Instance. Although it is attached like a RAW device, EBS essentially operates independently from EC2 and is connected through its own private network. EBS has two main types of disks: HDD and SSD, designed to achieve 99.999% availability by replicating data across 3 storage nodes within a single AZ. Some specific EC2 Instances are optimized for EBS performance (Optimized EBS Instances). By default, an EBS volume can only be attached to one EC2 Instance, but EC2 Instances running on the Nitro Hypervisor can use one EBS volume attached to multiple EC2 Instances (EBS Multi-Attach). EBS can be backed up by creating snapshots stored in S3 (Simple Storage Service). The first snapshot is a full backup, and all subsequent snapshots are incremental. Instance Store is a high-speed NVMe storage located on the physical node that hosts the EC2 virtual machines..\nInstance Store will lose all data when the EC2 instance is stopped. Instance Store does not lose data when the instance is restarted, but data can be lost if the instance crashes. Instance Store does not replicate data, so it is not recommended for storing important data. It is typically used in environments that require very high IOPS performance. When using Instance Store, data is often replicated to an EBS volume to ensure safety. EC2 user data is a script that runs once when an EC2 instance is provisioned from an AMI.\nDepending on the server\u0026rsquo;s operating system, we will use Bash shell scripts (Linux) or PowerShell (Windows). EC2 Metadata refers to information related to the EC2 instance itself, such as Private IP address, Public IP address, Hostname, Security Groups, and so on.\nEC2 Auto Scaling is a feature that supports increasing or decreasing the number of EC2 instances based on specific conditions (scaling policies).\nEC2 Auto Scaling can register EC2 instances with an Elastic Load Balancer. EC2 Auto Scaling operates across multiple AWS Availability Zones. EC2 Auto Scaling can support different pricing options. EC2 Autoscaling - EFS/FSx - Lightsail - MGN:\nEC2 offers four pricing options: On-Demand: Pay per hour/minute/second, only for what you use. Suitable for workloads that run occasionally, e.g., 6 hours a day. Reserved Instance: Commit to using for a 1–3 year term to get a discount, but limited to specific EC2 instance types/families. Savings Plans: Commit to usage for a 1–3 year term to get a discount, but may not be limited to specific EC2 instance type families. Spot Instance: Utilize spare capacity at a low price; however, AWS can terminate the instance with a 2-minute notice. Amazon Lightsail. Amazon Lightsail is a low-cost computing service (billed monthly, starting from $3.5/month). Additionally, each Lightsail instance includes a data transfer allowance (which is much cheaper than data transfer from EC2). Amazon Lightsail is suitable for light workloads, test/dev environments, and workloads that do not require high CPU usage continuously (less than 2 hours per day). Amazon Lightsail also supports backups via snapshots, similar to EC2. Amazon Lightsail runs in a separate VPC environment and can connect to other VPCs through VPC Peering. Amazon EFS/FSX. EFS EFS (Elastic File System) allows you to create NFSv4 network volumes and attach them to multiple EC2 instances simultaneously, with storage capacity scaling up to petabytes. EFS is Linux-only. Using EFS, you are billed only for the storage you use (whereas EBS charges based on allocated capacity). EFS can be configured to mount to on-premises environments via Direct Connect (DX) or VPN.x FSX FSx allows you to create NTFS volumes and attach them to multiple EC2 instances simultaneously using the SMB (Server Message Block) protocol. FSx supports both Windows and Linux. Using FSx, you are billed only for the storage you use (whereas EBS charges based on allocated capacity). FSx supports deduplication, which can reduce costs by 30–50% for typical use cases. AWS Application Migration Serveice(MGN):\nAWS Application Migration Service (MGN) is used to migrate and replicate physical servers into the AWS environment for the purpose of building a Disaster Recovery (DR) site. Application Migration Service(MGN) continuously replicates source servers to EC2 instances in the AWS account (asynchronously/synchronously). During replication, MGN uses staging machines that are much smaller in number and configuration compared to the source servers. During cutover, MGN stops the source servers and launches the EC2 instances on AWS. "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Amazon Simple Storage Service ( S3 ) - Access Point - Storage Class. S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier. Snow Family - Storage Gateway - Backup. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Amazon Simple Storage Service (S3) 2/010/2025 2/010/2025 2 - Amazon Simple Storage Service (S3) - Access Point 2/010/2025 2/010/2025 https://cloudjourney.awsstudygroup.com/ 3 - Amazon Simple Storage Service (S3) - Storage Class 2/010/2025 2/010/2025 https://cloudjourney.awsstudygroup.com/ 4 - S3 Static Website \u0026amp; CORS 3/010/2025 3/010/2025 https://cloudjourney.awsstudygroup.com/ 5 - Control Access - Object Key \u0026amp; Performance - Glacier 3/010/2025 3/010/2025 https://cloudjourney.awsstudygroup.com/ 6 - Snow Family - Storage Gateway - Backup 3/010/2025 4/010/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Amazon Simple Storage Service (S3):\nAmazon S3 is a place to store static data, meaning data that has been created and does not change afterward. S3 is suitable for data types that are written once and read many times (WORM - Write Once Read Many). Amazon S3 has no limit on the total amount of data stored. Each object cannot be larger than 5 TB. By default, data in Amazon S3 is replicated across 3 Availability Zones (AZs) within a single Region. Amazon S3 can (trigger events), allowing you to activate actions when an event occurs, such as uploading or deleting an object from a specific storage bucket. Amazon S3 is designed for 99.99999999% durability and 99.99% availability. Amazon S3 supports multipart upload for uploading large objects to a bucket. The program creates S3 buckets to store objects in Amazon S3. s3.amazonaws.com =\u0026gt; s3.amazonaws.com/capture.mp4 Amazon Simple Storage Service (S3) - Access Point: Amazon S3 Access Point is a feature that allows you to create unique connection points (unique hostnames) for applications, users, or groups.\nThe program can configure permissions when creating each access point. Amazon Simple Storage Service (S3) - Storage Class:\nAmazon S3 divides storage into multiple storage classes to help optimize costs. The storage classes of S3: S3 Standard: Frequently accessed data. S3 Standard IA: Infrequently accessed data. S3 Intelligent Tiering: Automatically moves objects between storage classes based on access patterns over time. S3 One Zone IA: Re-creatable data stored in a single zone, infrequently accessed but requiring quick access. Amazon Glacier / Deep Archive: Cold storage for data that is rarely accessed. Object Life Cycle Management (configurable data retention period):\nMove data within Amazon S3. A table defines the cases for moving data within an S3 bucket when stored data meets the configured time (days) criteria. Amazon Simple Storage Service (S3) - Static Website \u0026amp; CORS:\nAmazon S3 has a feature that allows hosting static websites (HTML, media, etc.), making it suitable for Single Page Applications (SPAs). A SPA is a web application that updates the current web page with new data from the web server using JavaScript or its frameworks such as AngularJS or ReactJS, instead of the browser’s default method of loading an entirely new page. Amazon S3 supports CORS. CORS is a mechanism that allows resources (fonts, JavaScript, etc.) of a web page to be requested from a domain different from the domain of that page. CORS stands for Cross-Origin Resource Sharing. Amazon Simple Storage Service (S3) - Control Access:\nAmazon S3 has two mechanisms for controlling access to buckets: S3 Access Control List (ACL) is a basic access control mechanism. However, if you are already using S3 ACL and see no need for changes, S3 ACLs are attached to S3 buckets and objects. It is not always clear when AWS will use this mechanism to grant access through specific permission types. S3 Bucket Policy and IAM Policy define object-level permissions by specifying conditions applied to the objects in the Resource section of the policy. This configuration applies to objects within the bucket. Unlike S3 ACLs, it also allows you to adjust conditions at the bucket level, giving you more flexibility in managing access. Amazon Simple Storage Service (S3) - Object Key \u0026amp; Performance:\nEach object in S3 is flat (not hierarchical) and is assigned an object key. Example: /image/sample.jpg, sample.jpg. Internally, S3 is divided into partitions, which are further split depending on the request load and the large number of S3 object keys (dots are used to help manage objects within a partition). S3 stores a key map (the key map helps route requests to partitions based on the hash of the object key prefix). To optimize S3 performance, you can use random prefixes (e.g., /fscd/img/sample.jpg instead of /img/sample.jpg). The goal of this approach is to distribute objects across as many partitions as possible, since S3 performance depends on the number of partitions. Amazon Simple Storage Service (S3) - Glacier:\nAmazon S3 Glacier is a low-cost storage option suitable for data that does not need to be accessed frequently and is intended for long-term storage. If you need fast or frequent access, you should choose Amazon S3 instead. When storing data in Amazon S3 Glacier, you cannot access it directly; you must retrieve the data back to an S3 bucket first. There are three retrieval options with different access times: Expedited Retrieval: Completed within 1–5 minutes. Standard Retrieval: Completed within 3–5 hours. Bulk Retrieval: Completed within 5–12 hours. Snow Family:\nSnowball: This service supports migrating data from an on-premises environment to AWS at a scale of up to Petabytes (PB). Each Snowball can hold up to 80 Terabytes (TB). The Snowball device will be shipped to the AWS region you select to store data in S3 or Glacier. The program uses the Snowball Client on the local machine to collect data and then transfer it. Snowball Edge: This service supports migrating data from an on-premises environment to AWS at a scale of up to Petabytes (PB). Each Snowball Edge can hold up to 100 Terabytes (TB). The Snowball Edge device will be shipped to the AWS region you select to store data in Glacier or S3. The program uses the Snowball Client on the local machine to collect data and then transfer it. Snowball Edge is a device used to back up data from resources across the entire system when importing into the device. Snowmobile: This service supports migrating data from an on-premises environment to AWS at a scale of up to 100 Petabytes (PB). Each Snowmobile can hold up to 100 PB. The Snowmobile will be transported to the AWS region you select to store data in services such as S3 or Glacier. Storage Gateway:\nAWS Storage Gateway is a hybrid storage solution that combines AWS storage capacity with on-premises storage. It is designed to scale seamlessly with cloud storage services to help store large amounts of data for long-term retention. AWS Storage Gateway supports three main storage methods: file, volume, and tape: File Gateway allows you to store and retrieve objects in Amazon S3 using the NFS or SMB file protocol. Data written through this file gateway can be accessed directly in S3. Volume Gateway provides storage using the iSCSI protocol. Data in the volumes is stored in Amazon S3. To access iSCSI volumes in AWS, you can create EBS snapshots (optionally through AWS Backup) to generate EBS volumes. Tape Gateway enables data backup using the VTL (iSCSI) protocol, virtual tape drives, and virtual tapes. Virtual tape data is stored in Amazon S3 or Glacier. Backup:\nAWS Backup is a service that manages backup tasks. We can configure and schedule backups (backup schedule), set retention periods (backup retention), and monitor backup activities for AWS resources, including: Amazon EBS Amazon EC2 Amazon RDS databases Amazon DynamoDB databases Amazon EFS AWS Storage Gateway volumes "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Explore the security services on AWS. Amazon Identity and Access Management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Amazon Cognito - AWS Organization 05/10/2025 05/10/2025 2 - AWS Identity Center - Amazon Key Management Service 05/10/2025 05/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - AWS Security Hub 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Shared Responsibility Model:\nThe shared responsibility model: When using cloud-based services, security responsibilities are shared between the customer and the service provider. Customers are responsible for protecting their data, applying best practices, and using services to ensure the security of virtual machines, the hypervisor, or database servers. Security responsibilities vary depending on the service layer: Basic services (IaaS) Managed server services (PaaS) Fully managed services (AWS) Amazon Identity and Access Management (IAM):\nRoot account: This account has full access to all AWS services and resources, and can grant additional permissions to resources.\nAccount information: Linked (when registering the account), Not restricted in permissions.\nBest Practices: Create and use an IAM Administrator User, lock the root user credentials (shared users), and ensure domain information is renewed via the root user’s email.\nIAM is a service that helps you control access to services and resources within your AWS account. IAM allows you to create multiple user accounts (IAM users) with different credentials and permission levels.\nIAM Principal is an entity that can perform actions on resources within your AWS account.\nAWS account and root user IAM users Federated users (using web identity or SAML federation) IAM roles Assumed-role sessions AWS services Anonymous users (not recommended) IAM Users are not separate AWS accounts. IAM users can be managed through the Management Console or by using an access key/secret key for programmatic access (AWS CLI or AWS SDK).\nBy default, an IAM User has no permissions when created. IAM Users should not be used to manage access to other applications or systems. Permissions for IAM Users are primarily assigned by attaching IAM Policies to the IAM User. For easier management, permissions are often assigned by grouping multiple IAM Users into an IAM Group. An IAM Group cannot be a member of another IAM Group. IAM Policy: There are two types of IAM policies\nIdentity-based Policy: Attached to an IAM Principal. Resource-based Policy: Attached to an AWS Resource. The way IAM permissions are enforced always prioritizes Deny over Allow. If there is an explicit Deny, then even if another IAM Policy grants Allow, the result will still be Deny.\nIAM Role:\nAllows defining a set of permissions to access resources (by attaching an IAM Policy to an IAM Role). An IAM Role does not have credentials to access the Management Console or AWS CLI/SDK. When an IAM User wants to use an IAM Role, the IAM User can assume the IAM Role. Once the role is assumed, the user’s permissions are replaced by the permissions granted to the IAM Role. In addition, temporary security credentials are issued to the IAM User or an AWS Service to access AWS services. Assuming a role works with AWS STS – Security Token Service, which generates temporary credentials (similar to an access key). For a user to assume an IAM Role, the role must be attached to a resource-based IAM policy, called an IAM Role trust policy, which defines who can use the IAM Role. IAM Roles are commonly used to follow the principle of least privilege, for example by granting permissions to other AWS accounts to access resources in the current AWS account. Besides being used by IAM Users, IAM Roles can also be used to grant AWS Services access to AWS resources. A common use case is granting permissions to applications running inside compute services (such as EC2) through IAM Roles. Amazon Cognito - AWS Organization:\nAmazon Cognito: A managed AWS service that provides authentication, authorization, and user management for web and mobile applications. Users can sign in directly with a username and password or through a third party such as Facebook, Amazon, or Google. The two main components of Amazon Cognito are User Pool and Identity Pool: User Pool stores user information and provides sign-up and sign-in options for application users. Identity Pool grants users access to other AWS services. AWS Organizations: AWS Organizations helps centrally manage and govern an environment that consists of multiple AWS accounts. AWS Organizations can create new AWS accounts, allocate resources, organize AWS accounts into Organizational Units (OU), and simplify centralized billing (consolidated billing). AWS Organizations can apply Service Control Policies (SCPs) to OUs or AWS accounts. SCPs establish permission boundaries for IAM Users or IAM Roles within those OUs or accounts. SCPs provide the ability to implement deny-based policies. AWS Identity Center (SSO):\nAWS Identity Center helps manage access to AWS accounts and external applications. The identity source can be a user directory within AWS Identity Center or linked to Active Directory (AWS Managed Microsoft AD, on-premises Microsoft AD via two-way trust, or AD Connector). Permission Sets define the access level of Users and Groups to AWS accounts within an AWS Organization. The permissions are stored in AWS Identity Center and provisioned to AWS accounts as IAM Roles. You can assign multiple permissions to a single User. Amazon Key Management Service(KMS):\nAWS Key Management Service (KMS) helps create and manage encryption keys, used for encrypting and decrypting data on AWS. Encryption keys always remain within AWS KMS, ensuring compliance with the FIPS 140-2 standard. CMK (Customer Managed Key) serves as the primary resource in AWS KMS. A CMK can be up to 4KB in size. However, in practice, CMKs are mainly used to generate, encrypt, and decrypt Data Keys—keys used outside of AWS KMS to encrypt data. AWS Security Hub: A service that enables standardized security checks based on security standards and best practices.\nSecurity Hub runs continuously, monitoring services in the AWS account and performing security checks based on AWS best practices and industry standards (e.g., PCI DSS). Security Hub provides findings in the form of scores and helps standardize the identification of accounts and resources that may require attention. "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Understand and set up the necessary resources for learning basic cloud on AWS.\nWeek 2: Getting familiar with Amazon VPC (Virtual Private Cloud) and VPN - Direct Connect - Load Balancer - Extra Resources.\nWeek 3: Understand how to run and connect to AWS virtual machine services.\nWeek 4: Virtual Machine (VM) service on AWS.\nWeek 5: Cloud storage service on AWS.\nWeek 6: Security service on AWS.\nWeek 7: Database services on AWS.\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Team Super Beast Warrior(SBW) Batch-based Clickstream Analytics Platform 1. Executive Summary This project aims to design and implement a Batch-based Clickstream Analytics Platform for a computer and accessories e-commerce website using AWS Cloud Services. The system collects user interaction data (such as clicks, searches, and page visits) from the website and stores it in Amazon S3 as raw logs. Every hour, Amazon EventBridge triggers AWS Lambda functions to process and transform the data before loading it into a data warehouse hosted on Amazon EC2.\nThe processed data is visualized through R Shiny dashboards, providing store owners with business insights such as customer behavior patterns, product popularity, and website engagement trends.\nThis architecture focuses on batch analytics, ETL pipelines, and business intelligence while ensuring security, scalability, and cost efficiency by leveraging AWS managed services.\n2. Problem Statement What’s the Problem? E-commerce websites generate a large volume of clickstream data—including product views, cart actions, and search activities—that contain valuable business insights.\nHowever, small and medium-sized stores often lack the infrastructure and expertise to collect, process, and analyze this data effectively.\nAs a result, they face difficulties in:\nUnderstanding customer purchasing behavior Identifying top-performing products Optimizing marketing campaigns and website performance Making data-driven inventory and pricing decisions The Solution This project introduces an AWS-based batch clickstream analytics system that automatically collects user interaction data from the website every hour, processes it through serverless functions, and stores it in a central data warehouse on Amazon EC2.\nThe results are visualized using R Shiny dashboards, enabling store owners to gain actionable insights into customer behavior and improve overall business performance.\nBenefits and Return on Investment Data-driven decision making: Discover customer preferences, popular products, and shopping trends. Scalable and modular design: Easily extendable to handle more users or additional data sources. Cost-efficient batch processing: Reduces continuous compute costs by operating on a scheduled, hourly basis. Business insight enablement: Empowers store owners to optimize sales strategies and improve revenue using evidence-based analytics. 3. Solution Architecture AWS Services Used Amazon Cognito: Handles user authentication and authorization for both administrators and website customers, ensuring secure access to the e-commerce platform. Amazon S3: Acts as a centralized data storage layer — hosting the static website front-end and storing raw clickstream logs collected from user interactions. It also temporarily holds batch files before they are processed and transferred to the data warehouse. Amazon CloudFront: Distributes static website content globally with low latency, improving user experience and caching resources close to customers. Amazon API Gateway: Serves as the main entry point for incoming API calls from the website, enabling secure data submission (such as clickstream or browsing activity) into AWS. AWS Lambda: Executes serverless functions to preprocess and organize clickstream data uploaded to S3. It also handles scheduled data transformation jobs triggered by EventBridge before loading them into the data warehouse. Amazon EventBridge: Schedules and orchestrates batch workflows — for example, triggering Lambda functions every hour to process and move clickstream data from S3 into the EC2 data warehouse. Amazon EC2 (Data Warehouse): Acts as the data warehouse environment, running PostgreSQL or another relational database for batch analytics, trend analysis, and business reporting. R Shiny (on EC2): Hosts interactive dashboards that visualize batch-processed insights, helping the business explore customer behavior, popular products, and sales opportunities. AWS IAM: Manages access permissions and policies to ensure that only authorized users and AWS components can interact with data and services. Amazon CloudWatch: Collects and monitors metrics, logs, and scheduled job statuses from Lambda and EC2 to maintain system reliability and performance visibility. Amazon SNS: Sends notifications or alerts when batch jobs complete, fail, or encounter errors, ensuring timely operational awareness. 4. Technical Implementation End-to-end data flow Auth (Cognito). Browser authenticates with Amazon Cognito (Hosted UI or JS SDK). ID token (JWT) is stored in memory; SDK attaches Authorization: Bearer \u0026lt;JWT\u0026gt; for API calls. Static web (CloudFront + S3). SPA/assets hosted on S3; CloudFront in front with OAC, gzip/brotli, HTTP/2, WAF managed rules. The page loads a tiny analytics SDK that collects events and sends to API Gateway (below). Event ingest (API Gateway). POST /v1/events (HTTP API). CORS locked to site origin; JWT authorizer validates Cognito token (or API key for anon flows). Requests forwarded to Lambda. Security \u0026amp; Ops. IAM least-privilege for every component. CloudWatch logs/metrics/alarms on API 5xx, Lambda errors, throttles, Shiny health. SNS notifies on alarms \u0026amp; DLQ growth. Processing \u0026amp; storage (Lambda → DynamoDB(datalake) → PostgreSQL on EC2(data warehouse)→ Shiny). Lambda validates/enriches events and writes to DynamoDB (session/event tables). A small ETL job on EC2 periodically compacts/aggregates DynamoDB data into a curated store (Postgres or DuckDB) on the EC2 data-warehouse node. R Shiny Server (on EC2) reads curated tables and renders dashboards for admins. Data Contracts \u0026amp; Governance Event JSON (ingest)\n{ \u0026#34;event_id\u0026#34;: \u0026#34;uuid-v4\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2025-10-18T12:34:56.789Z\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;view|click|search|add_to_cart|checkout|purchase\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;uuid-v4\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;cognito-sub-or-null\u0026#34;, \u0026#34;anonymous_id\u0026#34;: \u0026#34;stable-anon-id\u0026#34;, \u0026#34;page_url\u0026#34;: \u0026#34;https://site/p/123\u0026#34;, \u0026#34;referrer\u0026#34;: \u0026#34;https://google.com\u0026#34;, \u0026#34;device\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mobile|desktop|tablet\u0026#34; }, \u0026#34;geo\u0026#34;: { \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;city\u0026#34;: null }, \u0026#34;ecom\u0026#34;: { \u0026#34;product_id\u0026#34;: \u0026#34;sku-123\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Shoes\u0026#34;, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;price\u0026#34;: 79.99, \u0026#34;qty\u0026#34;: 1, }, \u0026#34;props\u0026#34;: { \u0026#34;search_query\u0026#34;: \u0026#34;running shoes\u0026#34; }, } PII: never send name/email/phone; any optional identifier is hashed in Lambda. Behaviour: generate anonymous_id once, maintain session_id (roll after 30 min idle), send via navigator.sendBeacon with fetch retry fallback; optional offline buffer via IndexedDB. S3 raw layout \u0026amp; retention\nBucket: s3://clickstream-raw/ Object format: NDJSON, optionally GZIP. Partitioning: year=YYYY/month=MM/day=DD/hour=HH/ → events-\u0026lt;uuid\u0026gt;.ndjson.gz Optional manifest per batch: processed watermark, object list, record counts, hash. Lifecycle: raw → (30 days Standard/IA) → (365+ days Glacier/Flex). Idempotency: maintain a compact staging table in PostgreSQL (or a small S3 key-value manifest) to track last processed object/batch and prevent double-load. Frontend SDK (Static site on S3 + CloudFront) Instrumentation\nTiny JS snippet loaded site-wide (defer). Generates anonymous_id once and keeps session_id in localStorage; session rolls after 30 minutes of inactivity. Sends events via navigator.sendBeacon; fallback to fetch with retry \u0026amp; jitter. Auth context\nIf user signs in with Cognito, include user_id = idToken.sub to enable logged-in funnels. Offline durability\nOptional Service Worker queue: when offline, buffer events in IndexedDB and flush on reconnect. Ingestion API (API Gateway → Lambda) API Gateway (HTTP API)\nRoute: POST /v1/events. JWT authorizer (Cognito user pool). For anonymous pre-login events, use an API key usage-plan with strict rate limits. WAF: AWS Managed Core + Bot Control; block non-site origins via strict CORS. Lambda (Node.js or Python)\nValidate against JSON Schema (ajv/pydantic). Idempotency: check recent event_id in a small DynamoDB table or in-memory cache w/ TTL. Enrichment: derive date/hour, parse UA, infer country from CloudFront-Viewer-Country if present. Persist: PutItem to events, UpdateItem to bump sessions.last_seen and page_count. Failure path: publish to SQS DLQ; alarm via SNS if DLQ depth \u0026gt; 0. Batch Buffer (S3) Purpose: cheap, durable buffer for batch analytics. Write pattern: small per-request objects or micro-batches (e.g., 1–5 MB each) with GZIP. Optional compactor merges into ≥64MB files for efficient reads. Read pattern: ETL Lambda scans only new partitions/objects since the last watermark. Schema-on-read: ETL applies schema, handles late-arriving data by reprocessing a small sliding window (e.g., last 2 hours) to correct sessions. EC2 “data warehouse” node Purpose: run ETL + host the curated analytical store that Shiny queries. Two choices:\nPostgres on EC2 (recommended if team prefers SQL/window functions) Instance: t3.small/t4g.small; gp3 50–100GB. Schema: fact_events, fact_sessions, dim_date, dim_product. Security: within VPC private subnet; access via ALB/SSM Session Manager; automated daily snapshots to S3 ETL (Lambda, batch via EventBridge cron) Trigger: rate(5 minutes) / cron(\u0026hellip;) depending on cost \u0026amp; freshness. Steps: list new S3 objects → read → validate/dedupe → transform (flatten nested JSON, cast types, add ingest_date, session_window_start/end) → upsert into Postgres using COPY to temp tables + merge, hoặc batched INSERT \u0026hellip; ON CONFLICT. Networking: Lambda attached to VPC private subnets to reach EC2 Postgres security group. R Shiny Server on EC2 (admin analytics) Server\nEC2 (t3.small/t4g.small) with: R 4.4+, Shiny Server (open-source), Nginx reverse proxy, TLS via ACM/ALB or Let’s Encrypt. IAM instance profile (no static keys). Security group allows HTTPS from office/VPN or Cognito-gated admin site. App (packages)\nshiny, shinydashboard/bslib, plotly, DT, dplyr, DBI + RPostgres or duckdb, lubridate. If querying DynamoDB directly for small cards, use paws.dynamodb (optional). Dashboards\nTraffic \u0026amp; Engagement: DAU/MAU, sessions, avg pages, bounce proxy. Funnels: view→add_to_cart→checkout→purchase with stage conversion \u0026amp; drop-off. Product Performance: views, CTR, ATC rate, revenue by product/category. Acquisition: referrer, campaign, device, country. Reliability: Lambda error rate, DLQ depth, ETL lag, data freshness. Caching\nQuery results cached in-process (reactive values) or materialized by ETL; cache keys by date range and filters Security baseline IAM\nIngest Lambda: s3:PutObject to raw bucket (scoped to prefix), s3:ListBucket on needed prefixes. ETL Lambda: s3:GetObject/ListBucket on raw prefixes; permission to fetch secrets from SSM Parameter Store; no broad S3 access. EC2 roles: read/write only to its own DB/volumes; optional read to S3 for backups. Shiny EC2: no write to S3 raw; read-only to Postgres as needed Network\nPlace EC2 in private subnets; public access through ALB (HTTPS 443). Lambda for ETL joins the VPC to reach Postgres; SG rules least-priv (Postgres port from ETL SG only). No wide 0.0.0.0/0 to DB ports. Data\nEncrypt EBS (KMS), S3 server-side encryption, RDS/PG TLS, secrets in SSM Parameter Store. No PII in events; retention: raw S3 90–365 days (lifecycle), curated Postgres per business policy Observability \u0026amp; alerting CloudWatch metrics/alarms\nAPI Gateway 5xx/latency, Lambda errors/throttles, DLQ depth, DynamoDB throttles, ETL job exit code/lag, Shiny health check. SNS topics: on-call email/SMS/Slack webhook.\nStructured logs: JSON logs from Lambda \u0026amp; ETL (request_id, event_type, status, ms, error_code).\nWatermark tracking: custom metric “DW Freshness (minutes since last successful upsert)”.\nCost Controls (stay near Free/low tier) Use HTTP API (cheaper), minimal Lambda memory (256–512MB), compress requests. Batch over realtime: S3 as buffer eliminates DynamoDB write/read costs. S3 lifecycle: Standard → Standard-IA/Intelligent-Tiering → Glacier for older raw; enable GZIP to cut storage/transfer. Tune ETL cadence (e.g., 15–60 min) and process only new objects; compact small files into bigger chunks to reduce read I/O. Single small EC2 for Shiny + DW at start; scale vertically or split later. AWS Budgets with SNS alerts (actual \u0026amp; forecast). Deliverables Analytics SDK (TypeScript) with sessionization + beacon + optional offline queue. API/Lambda (ingest) with validation, enrichment, idempotency hints, DLQ. S3 raw bucket spec (prefixing/partitioning, compression, lifecycle) + optional compactor. ETL Lambda (batch) + EventBridge cron + watermarking + upsert strategy to PostgreSQL. PostgreSQL schema (fact_events, fact_sessions, dims) + indexes + vacuum/maintenance plan. R Shiny dashboard app (5 modules) + Nginx/ALB TLS setup. Runbook: alarms, on-call, backups, disaster recovery, freshness SLO, cost guardrails 5. Timeline \u0026amp; Milestones Project Timeline Month 1 – Learning \u0026amp; Preparation Study a wide range of AWS services including compute, storage, analytics, and security. Understand key concepts of cloud architecture, data pipelines, and serverless computing. Conduct team meetings to align project goals and assign responsibilities.\nMonth 2 – Architecture Design \u0026amp; Prototyping Design the overall project architecture and define data flow between components. Set up initial AWS resources such as S3, Lambda, API Gateway, EventBridge, and EC2. Experiment with open-source tools for visualization and reporting. Test sample codes and validate the data ingestion and processing pipeline.\nMonth 3 – Implementation \u0026amp; Testing Implement the full architecture based on the approved design. Integrate all AWS services and ensure system reliability. Conduct performance and functionality testing. Finalize documentation and prepare the project for presentation.\n6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator. Or you can download the Budget Estimation File.\nInfrastructure Costs AWS Services\nAmazon Cognito(User Pools): 0.10 USD/monthly(1 Number of monthly active users (MAU), 1 Number of monthly active users (MAU) who sign in through SAML or OIDC federation)\nAmazon S3\n3 Standard:0.17 USD/monthly(6 GB, 1,000 PUT requests, 1,000 GET requests, 6 GB Data returned, 6 GB Data scanned) Data Transfer: 0.00 USD/monthly(Outbound: 6 TB, Inbound: 6 TB) Amazon CloudFront(United States): 0.64 USD/monthly(6 GB Data transfer out to internet, 6 GB Data transfer out to origin, 10,000 requests Number of requests (HTTPS))\nAmazon API Gateway(HTTP APIs): 0.01 USD/monthly(10,000 requests for HTTP API requests units)\nAmazon Lambda(Service settings): 0.00 USD/monthly(1,000,000 requests, 512 MB)\nAmazon CloudWatch(APIs): 0.03 USD/monthly(100 metrics GetMetricData, 1,000 metrics GetMetricWidgetImage, 1,000 requests API)\nAmazon SNS(Service settings): 0.02 USD/monthly(1,000,000 requests, 100,000 calls HTTP/HTTPS Notifications, 1,000 calls EMAIL/EMAIL-JSON Notifications, 100,000,000 notifications QS Notifications, 100,000,000 deliveries Amazon Web Services Lambda, 100,000 notifications Amazon Kinesis Data Firehose)\nAmazon EC2(EC2 specifications): 1.68 USD/monthly(1 instances, 730 Compute Savings Plans)\nAmazon EventBridge: 0.00 USD/monthly(1,000,000 events(Number of AWS management events) EventBridge Event Bus - Ingestion)\nTotal: 2.65 USD/month, 31.8 USD/12 months\n7. Risk Assessment Risk Likelihood Impact Mitigation Strategy High costs exceeding the estimated budget Medium High Closely monitor and calculate all potential AWS expenses. Limit the use of high-cost AWS services and replace them with simpler, cost-effective alternatives that provide similar functionality. Potential issues during data transfer or service integration between AWS components Medium Medium Perform step-by-step validation before going live. Conduct early testing, use managed AWS services, and continuously monitor performance through Amazon CloudWatch. Data collection or processing risks (e.g., excessive user interactions, network instability, missing or duplicated events) High Medium Apply data validation, temporary buffering, and schema enforcement to ensure consistency. Use structured logging and alarms to detect and resolve ingestion errors. Low or no user adoption of the analytics dashboard Low High Conduct internal training sessions and leverage existing communication channels to raise awareness. Encourage adoption by showcasing the system’s practical benefits and actionable insights. 8. Expected Outcomes Understanding Customer Behavior and Journey The system records the entire customer journey — including which pages users visit, which products they view, how long they stay, and where they exit the site.\nBy analyzing session duration, bounce rate, and navigation paths, businesses can evaluate user engagement and the overall experience.\nThis provides a reliable data foundation for improving website interface, optimizing page layout, and enhancing overall customer satisfaction.\nIdentifying Popular Products and Consumer Trends Based on clickstream data collected and processed in AWS, the system identifies the most viewed and most purchased products.\nProducts that receive less attention are also tracked, enabling businesses to assess the effectiveness of product listings, adjust pricing or visuals, and plan inventory more effectively.\nFurthermore, the system supports discovering shopping trends across time periods, regions, or device types — allowing for data-driven and timely business decisions.\nOptimizing Marketing and Sales Strategies Customer behavior data is transformed into business insights and presented through R Shiny dashboards.\nWith these analytical results, businesses can:\nAccurately define target customer segments for marketing efforts Customize advertising and promotional campaigns for specific product groups or demographics Evaluate the effectiveness of marketing initiatives through measurable engagement and conversion indicators As a result, marketing and sales strategies become more evidence-based and precise, supporting better decision-making and improved business performance.\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Extending deployment pipelines with Amazon ECS blue green deployments and lifecycle hooks. The blog introduces how to extend application deployment pipelines with Amazon ECS blue/green deployments combined with lifecycle hooks. Blue/green enables safe traffic shifting between two environments, while lifecycle hooks (Lambda functions inserted into each deployment stage) add custom logic such as testing, validation, or manual approval. This allows businesses to gain better control over the deployment process, reduce risks, and easily roll back in case of issues.\nBlog 2 - Accelerate serverless testing with LocalStack integration in VS Code IDE The blog introduces the integration of LocalStack into AWS Toolkit for VS Code, enabling developers to easily test and debug serverless applications locally without deploying to AWS. This makes the development, integration testing, and deployment of serverless applications faster, more convenient, and more seamless directly within the IDE.\nBlog 3 - Build centralized cross-Region backup architecture with AWS Control Tower The blog explains how to build a centralized cross-Region backup architecture by integrating AWS Backup with AWS Control Tower. When this integration is enabled, Control Tower automatically creates centralized backup vaults in each Region within a “central backup” account, along with local vaults in each workload account. Using backup policies, you can configure local or cross-Region backups and apply them consistently across the entire organization—reducing operational complexity, ensuring compliance, and enhancing data resiliency.\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. Event 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders.\nDate \u0026amp; Time: 9:00 – 17:00, Thursday, 18 September 2025.\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/hoangvananh_AWSFCj/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]